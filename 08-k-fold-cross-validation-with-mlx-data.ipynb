{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae071b23-1f32-416c-8d76-6ce501567305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import numpy as np\n",
    "from mlx.data import datasets\n",
    "from datasets_utils import cifar100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93704da-be13-45c5-af7e-6f54b85471ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streamed_data(data, batch_size=0, shuffled=True):\n",
    "    def transform_image(x):\n",
    "        return x.astype(\"float32\") / 255.0\n",
    "\n",
    "    buffer = data.shuffle() if shuffled else data\n",
    "    stream = buffer.to_stream()\n",
    "    stream = stream.key_transform(\"image\", transform_image)\n",
    "    stream = stream.batch(batch_size) if batch_size > 0 else stream\n",
    "    return stream.prefetch(4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7804b452-c360-42e8-8303-7dda46a0e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.load_cifar100(train=True)\n",
    "test_data = datasets.load_cifar100(train=False)\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5506e2e-3e70-4021-acb0-0957f14aa45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition length: 10000\n",
      "Batch image shape: (1, 32, 32, 3)\n",
      "Batch label shape: (1,)\n",
      "Original data length: 50000\n"
     ]
    }
   ],
   "source": [
    "# partition train_data into 5 partitions and return the first partition (index 0)\n",
    "partitioned = train_data.partition(5, 0)\n",
    "print(f\"Partition length: {len(partitioned)}\")\n",
    "\n",
    "streamed_partition = get_streamed_data(partitioned, batch_size=1, shuffled=True)\n",
    "batch = next(streamed_partition)\n",
    "print(f\"Batch image shape: {batch['image'].shape}\\nBatch label shape: {batch['label'].shape}\")\n",
    "print(f\"Original data length: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b78a14-388d-4b31-9a23-9dc8ed4a444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_channel, input_width, conv_filters, output_dims):\n",
    "        super().__init__()\n",
    "        conv2d_kernel_size = 3\n",
    "        conv2d_stride = 1\n",
    "        conv2d_padding = 0\n",
    "\n",
    "        pool2d_kernel_size = 2\n",
    "        pool2d_stride = 2\n",
    "        pool2d_padding = 0\n",
    "\n",
    "        self.conv_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=input_channel,\n",
    "                out_channels=conv_filters,\n",
    "                kernel_size=(conv2d_kernel_size, conv2d_kernel_size),\n",
    "                stride=conv2d_stride,\n",
    "                padding=conv2d_padding\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=conv_filters,\n",
    "                out_channels=conv_filters,\n",
    "                kernel_size=(conv2d_kernel_size, conv2d_kernel_size),\n",
    "                stride=conv2d_stride,\n",
    "                padding=conv2d_padding\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool2d_kernel_size, stride=pool2d_stride, padding=pool2d_padding),\n",
    "        )\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm(conv_filters)\n",
    "\n",
    "        self.conv_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=conv_filters,\n",
    "                out_channels=conv_filters,\n",
    "                kernel_size=(conv2d_kernel_size, conv2d_kernel_size),\n",
    "                stride=conv2d_stride,\n",
    "                padding=conv2d_padding\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=conv_filters,\n",
    "                out_channels=conv_filters,\n",
    "                kernel_size=(conv2d_kernel_size, conv2d_kernel_size),\n",
    "                stride=conv2d_stride,\n",
    "                padding=conv2d_padding\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=pool2d_kernel_size, stride=pool2d_stride, padding=pool2d_padding),\n",
    "        )\n",
    "\n",
    "        self.batch_norm2 = nn.BatchNorm(conv_filters)\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(input_dims=5*5*conv_filters, output_dims=output_dims),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = mx.flatten(x, start_axis=1, end_axis=-1)\n",
    "        x = self.fully_connected(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1592fd05-0e65-426e-825e-3a53ecebc61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (conv_layer1): Sequential(\n",
       "    (layers.0): Conv2d(3, 128, kernel_size=(3,), stride=(1, 1), padding=(0, 0), dilation=1, bias=True)\n",
       "    (layers.1): ReLU()\n",
       "    (layers.2): Conv2d(128, 128, kernel_size=(3,), stride=(1, 1), padding=(0, 0), dilation=1, bias=True)\n",
       "    (layers.3): ReLU()\n",
       "    (layers.4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "  )\n",
       "  (batch_norm1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_layer2): Sequential(\n",
       "    (layers.0): Conv2d(128, 128, kernel_size=(3,), stride=(1, 1), padding=(0, 0), dilation=1, bias=True)\n",
       "    (layers.1): ReLU()\n",
       "    (layers.2): Conv2d(128, 128, kernel_size=(3,), stride=(1, 1), padding=(0, 0), dilation=1, bias=True)\n",
       "    (layers.3): ReLU()\n",
       "    (layers.4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
       "  )\n",
       "  (batch_norm2): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fully_connected): Sequential(\n",
       "    (layers.0): Linear(input_dims=3200, output_dims=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = len(cifar100.labels)\n",
    "INPUT_WIDTH = 32\n",
    "INPUT_CHANNEL = 3\n",
    "CONV_FILTERS = 128\n",
    "\n",
    "model = Model(\n",
    "    input_channel=INPUT_CHANNEL,\n",
    "    input_width=INPUT_WIDTH,\n",
    "    conv_filters=CONV_FILTERS,\n",
    "    output_dims=NUM_CLASSES\n",
    ")\n",
    "\n",
    "mx.eval(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724c5843-2397-4010-972a-6824f8651218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.optimizers as optim\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Experiment:\n",
    "    model: nn.Module\n",
    "    optimizer: optim.Optimizer\n",
    "    train_losses = []\n",
    "    train_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    epoch: int = 30\n",
    "    f1_score: float = 0.0\n",
    "    recall_score: float = 0.0\n",
    "    precision_score: float = 0.0\n",
    "    description: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4e7f4-5bb8-4003-964f-742db2476aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: 1 | Epoch: 1 | avg. Train loss 4.533 | avg. Train acc 0.106 | avg. Validation score 0.148 | Throughput: 5918.00 images/sec\n",
      "Experiment: 1 | Epoch: 2 | avg. Train loss 4.470 | avg. Train acc 0.176 | avg. Validation score 0.189 | Throughput: 5987.50 images/sec\n",
      "Experiment: 1 | Epoch: 3 | avg. Train loss 4.444 | avg. Train acc 0.206 | avg. Validation score 0.201 | Throughput: 5993.67 images/sec\n",
      "Experiment: 1 | Epoch: 4 | avg. Train loss 4.424 | avg. Train acc 0.228 | avg. Validation score 0.213 | Throughput: 6003.05 images/sec\n",
      "Experiment: 1 | Epoch: 5 | avg. Train loss 4.409 | avg. Train acc 0.242 | avg. Validation score 0.218 | Throughput: 5992.96 images/sec\n",
      "Experiment: 1 | Epoch: 6 | avg. Train loss 4.395 | avg. Train acc 0.259 | avg. Validation score 0.233 | Throughput: 5993.68 images/sec\n",
      "Experiment: 1 | Epoch: 7 | avg. Train loss 4.383 | avg. Train acc 0.271 | avg. Validation score 0.241 | Throughput: 5998.39 images/sec\n",
      "Experiment: 1 | Epoch: 8 | avg. Train loss 4.372 | avg. Train acc 0.283 | avg. Validation score 0.248 | Throughput: 5996.90 images/sec\n",
      "Experiment: 1 | Epoch: 9 | avg. Train loss 4.361 | avg. Train acc 0.296 | avg. Validation score 0.260 | Throughput: 5992.35 images/sec\n",
      "Experiment: 1 | Epoch: 10 | avg. Train loss 4.351 | avg. Train acc 0.305 | avg. Validation score 0.262 | Throughput: 5993.84 images/sec\n",
      "Experiment: 1 | Epoch: 11 | avg. Train loss 4.343 | avg. Train acc 0.313 | avg. Validation score 0.270 | Throughput: 5960.90 images/sec\n",
      "Experiment: 1 | Epoch: 12 | avg. Train loss 4.334 | avg. Train acc 0.325 | avg. Validation score 0.275 | Throughput: 5973.86 images/sec\n",
      "Experiment: 1 | Epoch: 13 | avg. Train loss 4.321 | avg. Train acc 0.338 | avg. Validation score 0.282 | Throughput: 5962.06 images/sec\n",
      "Experiment: 1 | Epoch: 14 | avg. Train loss 4.315 | avg. Train acc 0.346 | avg. Validation score 0.283 | Throughput: 5932.07 images/sec\n",
      "Experiment: 1 | Epoch: 15 | avg. Train loss 4.307 | avg. Train acc 0.354 | avg. Validation score 0.286 | Throughput: 5933.26 images/sec\n",
      "Experiment: 1 | Epoch: 16 | avg. Train loss 4.302 | avg. Train acc 0.359 | avg. Validation score 0.293 | Throughput: 5943.87 images/sec\n"
     ]
    }
   ],
   "source": [
    "import trainer\n",
    "\n",
    "# Create five experiments with each model having slightly different or even same\n",
    "# Hyperparameters: learning rate, convolutional filters, etc.\n",
    "# For the sake of fun :)\n",
    "\n",
    "exp1 = Experiment(\n",
    "    model=Model(input_channel=INPUT_CHANNEL,\n",
    "                input_width=INPUT_WIDTH,\n",
    "                output_dims=NUM_CLASSES,\n",
    "                conv_filters=32),\n",
    "    epoch=30,\n",
    "    optimizer=optim.Adam(learning_rate=0.0001),\n",
    ")\n",
    "\n",
    "exp2 = Experiment(\n",
    "    model=Model(input_channel=INPUT_CHANNEL,\n",
    "                input_width=INPUT_WIDTH,\n",
    "                output_dims=NUM_CLASSES,\n",
    "                conv_filters=32),\n",
    "    epoch=30,\n",
    "    optimizer=optim.Adam(learning_rate=0.0005),\n",
    ")\n",
    "\n",
    "exp3 = Experiment(\n",
    "    model=Model(input_channel=INPUT_CHANNEL,\n",
    "                input_width=INPUT_WIDTH,\n",
    "                output_dims=NUM_CLASSES,\n",
    "                conv_filters=64),\n",
    "    epoch=30,\n",
    "    optimizer=optim.Adam(learning_rate=0.0001),\n",
    ")\n",
    "\n",
    "exp4 = Experiment(\n",
    "    model=Model(input_channel=INPUT_CHANNEL,\n",
    "                input_width=INPUT_WIDTH,\n",
    "                output_dims=NUM_CLASSES,\n",
    "                conv_filters=64),\n",
    "    epoch=30,\n",
    "    optimizer=optim.Adam(learning_rate=0.0005),\n",
    ")\n",
    "\n",
    "exp5 = Experiment(\n",
    "    model=Model(input_channel=INPUT_CHANNEL,\n",
    "                input_width=INPUT_WIDTH,\n",
    "                output_dims=NUM_CLASSES,\n",
    "                conv_filters=128),\n",
    "    epoch=30,\n",
    "    optimizer=optim.Adam(learning_rate=0.0001),\n",
    ")\n",
    "\n",
    "experiments = [exp1, exp2, exp3, exp4, exp5]\n",
    "n_experiments = len(experiments)\n",
    "\n",
    "def get_partitions(partition_start, partition_num):\n",
    "    ranges = [i for i in range(partition_num)]\n",
    "    ranges.remove(partition_start)\n",
    "    return partition_start, ranges\n",
    "\n",
    "\n",
    "for experiment_no, experiment in enumerate(experiments):\n",
    "    validation_partition_idx, train_partitions = get_partitions(experiment_no, n_experiments)\n",
    "    for epoch in range(experiment.epoch):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        train_throughputs = []\n",
    "        for train_partition_idx in train_partitions:\n",
    "            train_partition = train_data.partition(n_experiments, train_partition_idx)\n",
    "            data = get_streamed_data(data=train_partition, batch_size=256, shuffled=True)\n",
    "            train_loss, train_acc, throughput = trainer.train_epoch(\n",
    "                experiment.model,\n",
    "                data,\n",
    "                experiment.optimizer,\n",
    "                epoch,\n",
    "                verbose=False,\n",
    "            )\n",
    "            train_losses.append(train_loss.item())\n",
    "            train_accs.append(train_acc.item())\n",
    "            train_throughputs.append(throughput.item())\n",
    "        # Begin validation\n",
    "        validation_partition = train_data.partition(n_experiments, validation_partition_idx)\n",
    "        validation_data = get_streamed_data(data=validation_partition, batch_size=256, shuffled=False)\n",
    "        validation_score = trainer.test_epoch(experiment.model, validation_data, epoch)\n",
    "\n",
    "        train_losses = mx.mean(mx.array(train_losses))\n",
    "        train_accs = mx.mean(mx.array(train_accs))\n",
    "        train_throughputs = mx.mean(mx.array(train_throughputs))\n",
    "    \n",
    "        # Append results\n",
    "        experiment.train_losses.append(train_losses.item())\n",
    "        experiment.train_accuracy.append(train_accs.item())\n",
    "        experiment.validation_accuracy.append(validation_score.item())\n",
    "\n",
    "        print(\" | \".join(\n",
    "                (\n",
    "                    f\"Experiment: {experiment_no+1}\",\n",
    "                    f\"Epoch: {epoch+1}\",\n",
    "                    f\"avg. Train loss {train_losses.item():.3f}\",\n",
    "                    f\"avg. Train acc {train_accs.item():.3f}\",\n",
    "                    f\"avg. Validation score {validation_score.item():.3f}\",\n",
    "                    f\"Throughput: {train_throughputs.item():.2f} images/sec\",\n",
    "                )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cefcd0-1b1e-4ed9-b2fb-c41e9ec4080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get precision, recall, and f1-score\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# import numpy as np\n",
    "\n",
    "# test_stream = get_streamed_data(data=test_data, batch_size=256, shuffled=False)\n",
    "\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "# model.eval()\n",
    "# for batch in test_stream:\n",
    "#     X, y = batch[\"image\"], batch[\"label\"]\n",
    "#     X, y = mx.array(X), mx.array(y)\n",
    "#     logits = model(X)\n",
    "#     prediction = mx.argmax(mx.softmax(logits), axis=1)\n",
    "#     y_true = y_true + y.tolist()\n",
    "#     y_pred = y_pred + prediction.tolist()\n",
    "    \n",
    "# y_true = np.array(y_true)\n",
    "# y_pred = np.array(y_pred)\n",
    "\n",
    "# precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "# recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "# f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "# print(f\"Precision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30d3da-b189-444f-800d-c97acc6d7de9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
